"""
CONTRÃ”LEUR - Page d'extraction KOSMOS
Architecture MVC
GÃ¨re la logique de la page d'extraction (lecture, navigation, outils)
"""
import datetime
import csv # AJOUT
import json
import sys
import cv2
import numpy as np
import subprocess
from pathlib import Path
from PyQt6.QtCore import QObject, pyqtSignal
from PyQt6.QtWidgets import QInputDialog, QApplication

# Ajout du chemin racine pour les imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class UnderwaterFilters:
    """
    Collection de filtres rapides (vectorisÃ©s) pour amÃ©liorer des images sous-marines.
    Les mÃ©thodes opÃ¨rent sur des frames BGR (numpy.ndarray uint8).
    """

    @staticmethod
    def correct_blue_dominance(frame: np.ndarray, factor: float = 0.12) -> np.ndarray:
        """
        RÃ©duit une dominante bleue en renforÃ§ant lÃ©gÃ¨rement les canaux R et G.
        :param factor: intensitÃ© de correction (0.12 => +12% sur R/G).
        """
        r, g, b = cv2.split(frame)
        r = cv2.add(r, (r * factor).astype(np.uint8))
        g = cv2.add(g, (g * factor).astype(np.uint8))
        corrected = cv2.merge((r, g, b))
        return np.clip(corrected, 0, 255).astype(np.uint8)

    @staticmethod
    def apply_gamma(frame: np.ndarray, gamma: float = 1.2) -> np.ndarray:
        """
        Correction gamma via table de correspondance.
        gamma > 1 Ã©claircit les tons moyens.
        """
        gamma = max(gamma, 0.01)
        inv_gamma = 1.0 / gamma
        table = np.array([(i / 255.0) ** inv_gamma * 255 for i in np.arange(256)]).astype("uint8")
        return cv2.LUT(frame, table)

    @staticmethod
    def enhance_contrast(frame: np.ndarray, clip_limit: float = 2.0, tile_grid: tuple[int, int] = (8, 8)) -> np.ndarray:
        """
        AmÃ©liore le contraste local via CLAHE sur la luminance (Y dans YCrCb).
        """
        ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)
        y, cr, cb = cv2.split(ycrcb)
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid)
        y = clahe.apply(y)
        merged = cv2.merge((y, cr, cb))
        return cv2.cvtColor(merged, cv2.COLOR_YCrCb2BGR)

    @staticmethod
    def denoise(frame: np.ndarray, h: float = 10.0) -> np.ndarray:
        return cv2.fastNlMeansDenoisingColored(frame, None, h, h, 7, 21)

    @staticmethod
    def sharpen(frame: np.ndarray) -> np.ndarray:
        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
        return cv2.filter2D(frame, -1, kernel)

    @staticmethod
    def apply_contrast_brightness(frame: np.ndarray, contrast: int, brightness: int) -> np.ndarray:
        """Ajuste le contraste et la luminositÃ©. contrast/brightness de -100 Ã  100."""
        alpha = 1.0 + contrast / 100.0  # Facteur de contraste
        beta = brightness  # DÃ©calage de luminositÃ©
        adjusted = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)
        return adjusted

    @staticmethod
    def apply_saturation(frame: np.ndarray, value: int) -> np.ndarray:
        """Ajuste la saturation. value de -100 Ã  100."""
        if value == 0: return frame
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        factor = 1.0 + value / 100.0
        s = np.clip(s * factor, 0, 255).astype(np.uint8)
        hsv = cv2.merge([h, s, v])
        return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    @staticmethod
    def apply_hue(frame: np.ndarray, value: int) -> np.ndarray:
        """Ajuste la teinte. value de -90 Ã  90."""
        if value == 0: return frame
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        h, s, v = cv2.split(hsv)
        # L'Ã©chelle de teinte dans OpenCV est 0-179
        h = (h.astype(np.int32) + value) % 180
        hsv = cv2.merge([h.astype(np.uint8), s, v])
        return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    @staticmethod
    def apply_temperature(frame: np.ndarray, value: int) -> np.ndarray:
        """Ajuste la tempÃ©rature de couleur. value de -100 (froid) Ã  100 (chaud)."""
        if value == 0: return frame
        # Convertir la valeur en un ajustement pour les canaux bleu et rouge
        blue_factor = 1.0 - (value / 200.0 if value < 0 else 0)
        red_factor = 1.0 + (value / 200.0 if value > 0 else 0)
        b, g, r = cv2.split(frame)
        b = np.clip(b * blue_factor, 0, 255).astype(np.uint8)
        r = np.clip(r * red_factor, 0, 255).astype(np.uint8)
        return cv2.merge([b, g, r])

    @staticmethod
    def apply_lut(frame: np.ndarray, lut: list) -> np.ndarray:
        """Applique une table de correspondance (Look-Up Table)."""
        if len(lut) != 256: return frame
        table = np.array(lut, dtype=np.uint8)
        return cv2.LUT(frame, table)

class ExtractionKosmosController(QObject):
    """
    ContrÃ´leur pour la page d'extraction.
    GÃ¨re les interactions entre la vue ExtractionView et le modÃ¨le ApplicationModel.
    """
    
    # Signal pour demander Ã  l'application principale de changer de page
    navigation_demandee = pyqtSignal(str)
    
    def __init__(self, model, parent=None):
        super().__init__(parent)
        self.model = model
        self.view = None # Sera dÃ©fini lors de l'initialisation de la vue
        
        # Ã‰tat local pour les corrections
        self.brightness = 0
        self.contrast = 0
        self.pending_capture_name = None # Pour stocker le nom de la capture
        
    def set_view(self, view):
        """Associe la vue Ã  ce contrÃ´leur"""
        self.view = view
        self.detached_window = None
        
        # Connecter tous les signaux de la vue ici
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.detach_requested.connect(self.on_detach_player)
            
        # Afficher la premiÃ¨re vidÃ©o conservÃ©e au lancement de la page Extraction
        if self.view and hasattr(self.view, 'view_shown'):
            self.view.view_shown.connect(self.load_first_video)

    def load_first_video(self):
        """
        Charge la premiÃ¨re vidÃ©o marquÃ©e comme "conservÃ©e" dans le lecteur.
        Cette mÃ©thode est appelÃ©e lorsque la page d'extraction devient visible.
        """
        if not self.model.campagne_courante:
            return

        videos_conservees = self.model.campagne_courante.obtenir_videos_conservees()
        if videos_conservees:
            premiere_video = videos_conservees[0]
            print(f"ğŸ“¹ Chargement de la premiÃ¨re vidÃ©o conservÃ©e : {premiere_video.nom}")
            self.charger_video_dans_lecteur(premiere_video)

    def load_initial_data(self):
        """
        Charge les donnÃ©es initiales dans la vue au dÃ©marrage.
        RÃ©cupÃ¨re la liste des vidÃ©os du modÃ¨le et met Ã  jour l'explorateur.
        """
        if not self.view:
            return

        # RÃ©cupÃ©rer les vidÃ©os de la campagne courante
        videos = self.model.obtenir_videos()
        
        # Formater pour la vue (l'explorateur attend une liste de dicts)
        videos_data = []
        for vid in videos:
            # GÃ©nÃ©rer la miniature de la vidÃ©o
            thumbnail_pixmap = self._generer_miniature_video(vid.chemin)
            color = "#00CBA9" if vid.est_conservee else "#FF6B6B"
            videos_data.append({
                'name': vid.nom,
                'thumbnail_pixmap': thumbnail_pixmap,
                'thumbnail_color': color
            })
            
        # Mettre Ã  jour la liste dans la vue
        self.view.update_video_list(videos_data)
        
        # Si une vidÃ©o Ã©tait dÃ©jÃ  sÃ©lectionnÃ©e dans le modÃ¨le, on la charge
        if self.model.video_selectionnee:
            self.charger_video_dans_lecteur(self.model.video_selectionnee)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GESTION DE LA NAVIGATION ET SÃ‰LECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def on_tab_changed(self, tab_name):
        """GÃ¨re le changement d'onglet via la navbar"""
        # Mapping des noms d'onglets vers les IDs de pages
        tabs_map = {
            "Fichier": "accueil",
            "Tri": "tri",
            "Extraction": "extraction",
            "Ã‰vÃ¨nements": "evenements"
        }
        if tab_name in tabs_map:
            self.navigation_demandee.emit(tabs_map[tab_name])

    def on_video_selected(self, video_name):
        """
        AppelÃ© quand une vidÃ©o est cliquÃ©e dans l'explorateur.
        Met Ã  jour le modÃ¨le et demande Ã  la vue de charger la vidÃ©o.
        """
        # Mettre Ã  jour le modÃ¨le
        video = self.model.selectionner_video(video_name)
        
        if video:
            self.charger_video_dans_lecteur(video)
        else:
            print(f"âŒ Erreur: VidÃ©o '{video_name}' non trouvÃ©e dans le modÃ¨le.")

    def _charger_metadonnees_propres_json(self, video):
        """Charge les mÃ©tadonnÃ©es propres (section 'video') depuis le JSON."""
        try:
            json_path = Path(video.chemin).parent / f"{video.dossier_numero}.json"
            if not json_path.exists():
                return False
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            video.metadata_propres.clear()
            
            def flatten_dict(section_data, prefix=''):
                for key, value in section_data.items():
                    if isinstance(value, dict):
                        flatten_dict(value, prefix=f"{prefix}{key}_")
                    else:
                        full_key = f"{prefix}{key}"
                        video.metadata_propres[full_key] = str(value) if value is not None else ""

            if 'video' in data:
                flatten_dict(data['video'])
            return True
        except Exception as e:
            print(f"âŒ Erreur lecture JSON propres (extraction): {e}")
            return False

    def _charger_metadonnees_communes_json(self, video):
        """Charge les mÃ©tadonnÃ©es communes ('system', 'campaign') depuis le JSON."""
        try:
            json_path = Path(video.chemin).parent / f"{video.dossier_numero}.json"
            if not json_path.exists(): return False

            with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)
            video.metadata_communes.clear()

            def flatten_dict(d, p=''):
                for k, v in d.items():
                    if isinstance(v, dict): flatten_dict(v, f"{p}{k}_")
                    else: video.metadata_communes[f"{p}{k}"] = str(v) if v is not None else ""
            
            if 'system' in data: flatten_dict(data['system'], "system_")
            if 'campaign' in data: flatten_dict(data['campaign'], "campaign_")
            return True
        except Exception as e:
            print(f"âŒ Erreur lecture JSON communes (extraction): {e}")
            return False

    def _charger_donnees_timeseries_csv(self, video):
        """Charge les donnÃ©es temporelles (temp, pression...) depuis le CSV."""
        video.timeseries_data = [] # RÃ©initialiser les donnÃ©es
        try:
            csv_path = Path(video.chemin).parent / f"{video.dossier_numero}.csv"
            if not csv_path.exists():
                print(f"âš ï¸ Fichier CSV non trouvÃ© pour la vidÃ©o: {csv_path}")
                self.view.video_player.set_timeseries_data([])
                return False

            with open(csv_path, 'r', encoding='utf-8') as f:
                # DÃ©tecter le dÃ©limiteur en lisant la premiÃ¨re ligne
                first_line = f.readline()
                delimiter = ';' if ';' in first_line else ','
                f.seek(0) # Revenir au dÃ©but du fichier
                reader = csv.DictReader(f, delimiter=delimiter)
                
                # --- NOUVELLE LOGIQUE DE SYNCHRONISATION BASÃ‰E SUR HMS ---
                
                def hms_to_seconds(hms_str):
                    """Convertit une chaÃ®ne 'HHhMMmSSs' en secondes totales."""
                    try:
                        parts = hms_str.lower().replace('s', '').split('h')
                        h = int(parts[0])
                        parts = parts[1].split('m')
                        m = int(parts[0])
                        s = int(parts[1])
                        return h * 3600 + m * 60 + s
                    except (ValueError, IndexError):
                        return None

                # Lire la premiÃ¨re ligne de donnÃ©es pour obtenir l'heure de dÃ©but
                all_rows = list(reader)
                if not all_rows:
                    return False

                start_hms_str = all_rows[0].get('HMS')
                start_total_seconds = hms_to_seconds(start_hms_str)

                if start_total_seconds is None:
                    print("âŒ Erreur: Impossible de lire l'heure de dÃ©but (colonne HMS) dans le CSV.")
                    return False

                # Mapper les noms de colonnes possibles vers les noms standard
                column_mapping = {
                    'pression': 'Pression',
                    'temperature': 'TempC',
                    'lux': 'Lux'
                }
                
                for row in all_rows:
                    processed_row = {}
                    current_hms_str = row.get('HMS')
                    current_total_seconds = hms_to_seconds(current_hms_str)

                    for standard_name, csv_name in column_mapping.items():
                        if csv_name in row and row[csv_name] and row[csv_name].strip():
                            value = row[csv_name].strip().replace(',', '.')
                            processed_row[standard_name] = value
                    
                    if current_total_seconds is not None:
                        delta_seconds = current_total_seconds - start_total_seconds
                        processed_row['timestamp_ms'] = int(delta_seconds * 1000)
                        video.timeseries_data.append(processed_row)

            print(f"âœ… DonnÃ©es CSV chargÃ©es pour {video.nom}: {len(video.timeseries_data)} points.")
            return True
        except Exception as e:
            print(f"âŒ Erreur lecture CSV (extraction): {e}")
            return False

    def charger_video_dans_lecteur(self, video):
        """PrÃ©pare les donnÃ©es de la vidÃ©o et met Ã  jour le lecteur de la vue"""
        if not self.view:
            return

        # --- AJOUT IMPORTANT : Recharger les mÃ©tadonnÃ©es depuis le JSON ---
        self._charger_metadonnees_propres_json(video)
        self._charger_metadonnees_communes_json(video)
        self._charger_donnees_timeseries_csv(video) # AJOUT
        # --- FIN AJOUT ---

        # PrÃ©parer les mÃ©tadonnÃ©es STATIQUES pour l'affichage.
        # On ne passe QUE le temps de dÃ©part. Le reste (temp, pression, lux)
        # sera gÃ©rÃ© dynamiquement par le lecteur Ã  partir des donnÃ©es CSV.
        metadata_display = {
            "time": video.start_time_str,
        }

        video_data = {
            'path': video.chemin,
            'metadata': metadata_display,
            'timeseries_data': video.timeseries_data # CORRECTION: Utiliser les donnÃ©es chargÃ©es
        }

        # Demander Ã  la vue de charger cette vidÃ©o
        self.view.update_video_player(video_data)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONTRÃ”LE DU LECTEUR
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def on_detach_player(self):
        """DÃ©tache le lecteur dans une nouvelle fenÃªtre"""
        if not self.view or not hasattr(self.view, 'video_player'):
            return
        
        # Importer la fenÃªtre dÃ©tachÃ©e
        from components.detached_player import DetachedPlayerWindow
        
        # Sauvegarder la rÃ©fÃ©rence au layout parent
        if not hasattr(self, 'video_player_parent_layout'):
            # Trouver le parent layout (normalement center_right_layout)
            self.video_player_parent_layout = self.view.video_player.parent().layout()
            self.video_player_parent_index = self.video_player_parent_layout.indexOf(self.view.video_player)
        
        # Retirer le lecteur de la vue principale
        video_player = self.view.video_player
        self.video_player_parent_layout.removeWidget(video_player)
        video_player.setParent(None)
        
        # CrÃ©er la fenÃªtre dÃ©tachÃ©e
        self.detached_window = DetachedPlayerWindow(video_player, parent=None)
        self.detached_window.closed.connect(self.on_reattach_player)
        self.detached_window.show()
        
        print("ğŸ—— Lecteur dÃ©tachÃ© dans une nouvelle fenÃªtre")

    def on_reattach_player(self):
        """RÃ©attache le lecteur Ã  la vue principale"""
        if not self.detached_window or not self.view:
            return
        
        # RÃ©cupÃ©rer le lecteur
        video_player = self.detached_window.video_player
        video_player.setParent(self.view)
        
        # RÃ©insÃ©rer dans le layout Ã  la bonne position
        if hasattr(self, 'video_player_parent_layout') and hasattr(self, 'video_player_parent_index'):
            self.video_player_parent_layout.insertWidget(
                self.video_player_parent_index, 
                video_player, 
                stretch=5
            )
        
        # Nettoyer
        self.detached_window.deleteLater()
        self.detached_window = None
        
        print("ğŸ”— Lecteur rÃ©attachÃ© Ã  la vue principale")
        
    def on_play_pause(self):
        """GÃ¨re le bouton Play/Pause"""
        print("â¯ï¸ Play/Pause demandÃ©")

    def on_position_changed(self, position):
        """GÃ¨re le changement de position (slider)"""
        pass

    def on_previous_video(self):
        """Passe Ã  la vidÃ©o prÃ©cÃ©dente dans la liste"""
        self._naviguer_video(-1)

    def on_next_video(self):
        """Passe Ã  la vidÃ©o suivante dans la liste"""
        self._naviguer_video(1)

    def on_rewind(self):
        """Recule de X secondes"""
        print("âª Retour arriÃ¨re")

    def on_forward(self):
        """Avance de X secondes"""
        print("â© Avance rapide")

    def _naviguer_video(self, direction):
        """Logique interne pour changer de vidÃ©o (prÃ©cÃ©dente/suivante)"""
        videos = self.model.obtenir_videos()
        if not videos or not self.model.video_selectionnee:
            return

        current_name = self.model.video_selectionnee.nom
        
        # Trouver l'index actuel
        try:
            current_idx = -1
            for i, v in enumerate(videos):
                if v.nom == current_name:
                    current_idx = i
                    break
            
            if current_idx != -1:
                new_idx = (current_idx + direction) % len(videos)
                new_video = videos[new_idx]
                # Simuler un clic pour dÃ©clencher toute la chaÃ®ne de mise Ã  jour
                self.on_video_selected(new_video.nom)
                
        except ValueError:
            pass

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CORRECTION D'IMAGE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def on_contrast_changed(self, value):
        """GÃ¨re le slider de contraste"""
        self.contrast = value
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('contrast_base', UnderwaterFilters.apply_contrast_brightness, value != 0 or self.brightness != 0, contrast=self.contrast, brightness=self.brightness)

    def on_brightness_changed(self, value):
        """GÃ¨re le slider de luminositÃ©"""
        self.brightness = value
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('contrast_base', UnderwaterFilters.apply_contrast_brightness, self.contrast != 0 or value != 0, contrast=self.contrast, brightness=self.brightness)

    def on_color_correction(self):
        """Ouvre ou applique la correction colorimÃ©trique automatique"""
        if not self.view or not hasattr(self.view, 'video_player'):
            return

        # Appliquer une chaÃ®ne de filtres par dÃ©faut
        self.view.video_player.toggle_filter('gamma', UnderwaterFilters.apply_gamma, True, gamma=1.2)
        self.view.video_player.toggle_filter('blue_correction', UnderwaterFilters.correct_blue_dominance, True, factor=0.15)
        self.view.video_player.toggle_filter('contrast', UnderwaterFilters.enhance_contrast, True, clip_limit=1.5)
        
        # Mettre Ã  jour l'Ã©tat des boutons de filtre dans le composant ImageCorrection
        self.view.image_correction.update_filter_buttons_state({
            'gamma': True,
            'blue_correction': True,
            'contrast': True,
            'denoise': self.view.video_player.is_filter_active('denoise'),
            'sharpen': self.view.video_player.is_filter_active('sharpen')
        })
        self.view.show_message("Correction automatique appliquÃ©e.", "success")

    def on_toggle_gamma(self, toggled):
        """Active ou dÃ©sactive la correction gamma."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('gamma', UnderwaterFilters.apply_gamma, toggled, gamma=1.2)

    def on_toggle_contrast(self, toggled):
        """Active ou dÃ©sactive l'amÃ©lioration du contraste."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('contrast', UnderwaterFilters.enhance_contrast, toggled, clip_limit=1.5)

    def on_toggle_denoise(self, toggled):
        """Active ou dÃ©sactive la rÃ©duction de bruit."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('denoise', UnderwaterFilters.denoise, toggled, h=10.0)

    def on_toggle_sharpen(self, toggled):
        """Active ou dÃ©sactive le filtre de nettetÃ©."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('sharpen', UnderwaterFilters.sharpen, toggled)

    def on_reset_filters(self):
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.reset_filters()

    def on_saturation_changed(self, value):
        """GÃ¨re le slider de saturation."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('saturation', UnderwaterFilters.apply_saturation, value != 0, value=value)

    def on_hue_changed(self, value):
        """GÃ¨re le slider de teinte."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('hue', UnderwaterFilters.apply_hue, value != 0, value=value)

    def on_temperature_changed(self, value):
        """GÃ¨re le slider de tempÃ©rature."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('temperature', UnderwaterFilters.apply_temperature, value != 0, value=value)

    def on_curve_changed(self, lut):
        """GÃ¨re le changement de la courbe tonale."""
        if self.view and hasattr(self.view, 'video_player'):
            self.view.video_player.toggle_filter('curve', UnderwaterFilters.apply_lut, True, lut=lut)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OUTILS D'EXTRACTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def on_screenshot(self):
        from PyQt6.QtWidgets import QMessageBox
        """Active le mode de sÃ©lection sur le lecteur vidÃ©o pour une capture d'Ã©cran."""
        if not self.model.video_selectionnee:
            self.view.show_message("Aucune vidÃ©o sÃ©lectionnÃ©e.", "warning")
            return
        
        if not self.view or not hasattr(self.view, 'video_player'):
            return
            
        # CrÃ©er une boÃ®te de dialogue pour demander le type de capture
        msg_box = QMessageBox(self.view)
        msg_box.setWindowTitle("Type de capture")
        msg_box.setText("Quel type de capture d'Ã©cran souhaitez-vous effectuer ?")
        msg_box.setIcon(QMessageBox.Icon.Question)
        
        # Ajouter les boutons personnalisÃ©s
        btn_full = msg_box.addButton("Image complÃ¨te", QMessageBox.ButtonRole.YesRole)
        btn_crop = msg_box.addButton("SÃ©lectionner une zone", QMessageBox.ButtonRole.NoRole)
        msg_box.addButton("Annuler", QMessageBox.ButtonRole.RejectRole)
        
        msg_box.exec()
        
        clicked_button = msg_box.clickedButton()
        
        if clicked_button == btn_full:
            # Capture de l'image complÃ¨te : on appelle directement grab_frame sans rectangle
            self.view.video_player.grab_frame(None)
        elif clicked_button == btn_crop:
            # SÃ©lection d'une zone : on active le mode de recadrage
            self.view.show_message("Dessinez un rectangle sur la vidÃ©o pour capturer une zone.", "info")
            self.view.video_player.start_cropping()

    def on_crop_area_selected(self, crop_rect):
        """Slot appelÃ© lorsque l'utilisateur a sÃ©lectionnÃ© une zone Ã  capturer."""
        # Demande au lecteur de capturer l'image, en lui passant la zone Ã  recadrer.
        self.view.video_player.grab_frame(crop_rect)

    def save_captured_frame(self, frame: 'QPixmap'):
        """
        ReÃ§oit le QPixmap (dÃ©jÃ  recadrÃ© si nÃ©cessaire), demande un nom Ã  l'utilisateur,
        puis sauvegarde l'image.
        """
        if not frame:
            self.view.show_message("Impossible de capturer l'image de la vidÃ©o.", "error")
            return

        # 1. Demander le nom de la capture maintenant, aprÃ¨s la sÃ©lection.
        capture_name, ok_pressed = QInputDialog.getText(
            self.view,
            "Nommer la capture",
            "Entrez le nom de la capture (sans extension) :",
        )

        if not ok_pressed or not capture_name:
            self.view.show_message("Capture annulÃ©e.", "info")
            return

        self.pending_capture_name = capture_name

        # 2. DÃ©finir le chemin de sauvegarde
        workspace = self.model.campagne_courante.workspace_extraction
        if not workspace:
            self.view.show_message("Dossier d'extraction non dÃ©fini pour la campagne.", "error")
            return

        # CrÃ©er un sous-dossier "captures" pour une meilleure organisation
        captures_dir = Path(workspace) / "captures"
        captures_dir.mkdir(parents=True, exist_ok=True)

        # 3. Utiliser le nom fourni par l'utilisateur
        filename = f"{self.pending_capture_name}.png"
        save_path = captures_dir / filename

        # 4. Sauvegarder l'image (qui est dÃ©jÃ  recadrÃ©e)
        try:
            frame.save(str(save_path), "png", -1)
            self.view.show_message(f"Capture enregistrÃ©e : {filename}", "success")
            print(f"ğŸ“¸ Capture d'Ã©cran enregistrÃ©e sous : {save_path}")
        except Exception as e:
            self.view.show_message(f"Erreur lors de la sauvegarde : {e}", "error")
            print(f"âŒ Erreur sauvegarde capture : {e}")
        finally:
            # RÃ©initialiser le nom pour la prochaine capture
            self.pending_capture_name = None
            
    def _export_video_with_filters(self, source_path, output_path, start_ms, end_ms):
        """
        Exporte une portion de vidÃ©o en appliquant les filtres actifs via OpenCV
        et en encodant via FFmpeg (pipe).
        """
        import cv2
        
        cap = cv2.VideoCapture(str(source_path))
        if not cap.isOpened():
            raise Exception("Impossible d'ouvrir la vidÃ©o source")
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        cap.set(cv2.CAP_PROP_POS_MSEC, start_ms)
        
        duration_ms = end_ms - start_ms
        duration_s = duration_ms / 1000.0
        frames_to_process = int(duration_s * fps)
        
        start_str = str(datetime.timedelta(milliseconds=start_ms))

        # Commande FFmpeg :
        # Input 0: Raw video from stdin (OpenCV)
        # Input 1: Audio from source file (cut with -ss and -t)
        cmd = [
            'ffmpeg', '-y',
            '-loglevel', 'error', # RÃ©duire la verbositÃ© pour Ã©viter le blocage du pipe stderr
            '-f', 'rawvideo',
            '-vcodec', 'rawvideo',
            '-s', f'{width}x{height}',
            '-pix_fmt', 'bgr24',
            '-r', str(fps),
            '-i', '-', 
            '-ss', start_str,
            '-i', str(source_path),
            '-t', str(duration_s),
            '-map', '0:v',
            '-map', '1:a?', # Audio optionnel
            '-c:v', 'libx264',
            '-preset', 'medium',
            '-crf', '23',
            '-c:a', 'aac',
            '-b:a', '128k',
            str(output_path)
        ]
        
        print(f"ğŸš€ Commande FFmpeg: {' '.join(cmd)}")
        
        creation_flags = 0
        if sys.platform == "win32":
            creation_flags = subprocess.CREATE_NO_WINDOW
            
        process = subprocess.Popen(
            cmd, 
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            creationflags=creation_flags
        )
        
        filters = []
        if self.view and hasattr(self.view, 'video_player'):
            filters = self.view.video_player.active_filters
            
        print(f"ğŸ¬ Export avec filtres ({len(filters)} actifs)...")
        
        try:
            count = 0
            while count < frames_to_process:
                # Garder l'interface rÃ©active
                QApplication.processEvents()
                
                ret, frame = cap.read()
                if not ret:
                    break
                    
                # Appliquer les filtres
                if filters:
                    for name, (filter_func, kwargs) in filters.items():
                        try:
                            frame = filter_func(frame, **kwargs)
                        except Exception as e:
                            print(f"âš ï¸ Erreur filtre {name}: {e}")
                
                # Ã‰crire dans le pipe
                try:
                    process.stdin.write(frame.tobytes())
                except IOError as e:
                    print(f"âŒ Erreur Ã©criture pipe: {e}")
                    break
                    
                count += 1
                
        finally:
            cap.release()
            if process.stdin:
                process.stdin.close()
            
            # Attendre la fin du processus et rÃ©cupÃ©rer stderr
            stdout_data, stderr_data = process.communicate()
            
            if process.returncode != 0:
                stderr_output = stderr_data.decode('utf-8', errors='replace') if stderr_data else "Erreur inconnue"
                print(f"âŒ Erreur FFmpeg (code {process.returncode}): {stderr_output}")
                raise Exception(f"Erreur lors de l'encodage FFmpeg: {stderr_output[-200:]}")
            else:
                print("âœ… Export FFmpeg terminÃ© avec succÃ¨s.")

    def on_recording(self):
        """DÃ©marre/ArrÃªte l'enregistrement d'un extrait"""
        if not self.view or not self.model.video_selectionnee:
            self.view.show_message("Aucune vidÃ©o sÃ©lectionnÃ©e.", "warning")
            return

        # Calculer la position actuelle en fonction de la frame courante
        video_thread = self.view.video_player.video_thread
        if video_thread.total_frames == 0:
            self.view.show_message("La durÃ©e de la vidÃ©o est inconnue.", "error")
            return
            
        current_pos_ms = int((video_thread.current_frame / video_thread.fps) * 1000)
        duration_ms = self.view.video_player.duration

        if duration_ms == 0:
            self.view.show_message("La durÃ©e de la vidÃ©o est inconnue.", "error")
            return

        # 1. DÃ©finir la plage de sÃ©lection initiale (position actuelle + 30s)
        initial_start_ms = current_pos_ms
        initial_end_ms = min(duration_ms, current_pos_ms + 30000)

        # 2. Ouvrir la nouvelle fenÃªtre d'Ã©dition
        from components.clip_editor_dialog import ClipEditorDialog
        dialog = ClipEditorDialog(
            self.model.video_selectionnee.chemin,
            initial_start_ms,
            initial_end_ms,
            self.view
        )
        
        accepted = dialog.exec()

        # 3. Si l'utilisateur a validÃ©, crÃ©er l'extrait final
        if not accepted:
            self.view.show_message("Enregistrement annulÃ©.", "info")
            return

        try:
            rec_name, final_start_ms, final_end_ms = dialog.get_values()
            
            final_start_str = str(datetime.timedelta(milliseconds=final_start_ms))
            final_duration_s = (final_end_ms - final_start_ms) / 1000

            recordings_dir = Path(self.model.campagne_courante.workspace_extraction) / "recordings"
            recordings_dir.mkdir(parents=True, exist_ok=True)
            final_output_path = recordings_dir / f"{rec_name}.mp4"

            self.view.show_message("Enregistrement de l'extrait final...", "info")
            
            # Utiliser la nouvelle mÃ©thode d'export avec filtres
            self._export_video_with_filters(
                self.model.video_selectionnee.chemin,
                final_output_path,
                final_start_ms,
                final_end_ms
            )
            
            self.view.show_message(f"Enregistrement '{rec_name}.mp4' sauvegardÃ© !", "success")
        except Exception as e:
            self.view.show_message(f"Erreur enregistrement final: {e}", "error")
                    
    def on_create_short(self):
        """CrÃ©e un short (extrait court format vertical ou spÃ©cifique)"""
        if not self.view or not self.model.video_selectionnee:
            self.view.show_message("Aucune vidÃ©o sÃ©lectionnÃ©e.", "warning")
            return

        # 1. Obtenir la position actuelle et la durÃ©e totale
        player = self.view.video_player
        video_thread = player.video_thread
        
        if video_thread.total_frames == 0:
            self.view.show_message("La durÃ©e de la vidÃ©o est inconnue.", "error")
            return
            
        current_pos_ms = int((video_thread.current_frame / video_thread.fps) * 1000)
        total_duration_ms = player.duration

        if total_duration_ms == 0:
            self.view.show_message("La durÃ©e de la vidÃ©o est inconnue.", "error")
            return

        # 2. NOUVEAU : Demander Ã  l'utilisateur de choisir la durÃ©e du short
        durations = ["10 secondes", "20 secondes", "30 secondes"]
        selected_duration_str, ok = QInputDialog.getItem(
            self.view,
            "Choisir la durÃ©e du Short",
            "Quelle durÃ©e pour votre short ?",
            durations,
            0,  # index par dÃ©faut
            False # non-Ã©ditable
        )

        if not ok:
            self.view.show_message("CrÃ©ation du short annulÃ©e.", "info")
            return

        # Extraire la durÃ©e en secondes (ex: "10 secondes" -> 10)
        clip_duration_s = int(selected_duration_str.split()[0])

        # 3. Calculer les temps de dÃ©but et de fin en fonction de la durÃ©e choisie
        start_ms = max(0, current_pos_ms - (clip_duration_s * 1000 // 2))
        end_ms = min(total_duration_ms, start_ms + (clip_duration_s * 1000))
        clip_duration_s = (end_ms - start_ms) / 1000

        # Convertir en format HH:MM:SS.ms pour ffmpeg
        start_time_str = str(datetime.timedelta(milliseconds=start_ms))

        # 3. DÃ©finir les chemins temporaires
        extraction_dir = Path(self.model.campagne_courante.workspace_extraction)
        shorts_dir = extraction_dir / "shorts"
        shorts_dir.mkdir(parents=True, exist_ok=True)
        
        temp_filtered_path = shorts_dir / f"~temp_filtered.mp4"
        temp_preview_path = shorts_dir / f"~preview_temp.mp4"

        # 4. CrÃ©er un aperÃ§u accÃ©lÃ©rÃ© avec filtres
        try:
            self.view.show_message("GÃ©nÃ©ration de l'aperÃ§u avec filtres...", "info")
            
            # Ã‰tape 1: GÃ©nÃ©rer le clip filtrÃ© Ã  vitesse normale
            end_ms = start_ms + int(clip_duration_s * 1000)
            self._export_video_with_filters(
                self.model.video_selectionnee.chemin,
                temp_filtered_path,
                start_ms,
                end_ms
            )
            
            # Ã‰tape 2: AccÃ©lÃ©rer ce clip pour l'aperÃ§u (x2)
            cmd_preview = [
                'ffmpeg', '-y',
                '-i', str(temp_filtered_path),
                '-vf', 'setpts=0.5*PTS',
                '-af', 'atempo=2.0',
                '-preset', 'ultrafast',
                '-crf', '28',
                str(temp_preview_path)
            ]
            
            creation_flags = 0
            if sys.platform == "win32":
                creation_flags = subprocess.CREATE_NO_WINDOW
                
            subprocess.run(cmd_preview, check=True, capture_output=True, text=True, creationflags=creation_flags)
            
        except Exception as e:
            self.view.show_message(f"Erreur crÃ©ation aperÃ§u: {e}", "error")
            # Nettoyage en cas d'erreur
            if temp_filtered_path.exists(): temp_filtered_path.unlink()
            return

        # 5. Afficher la boÃ®te de dialogue d'aperÃ§u
        from components.short_preview_dialog import ShortPreviewDialog
        preview_dialog = ShortPreviewDialog(str(temp_preview_path), self.view)
        
        accepted = preview_dialog.exec()

        try:
            # 6. Si l'utilisateur a cliquÃ© sur "Enregistrer" et entrÃ© un nom
            if accepted:
                short_name = preview_dialog.get_short_name()

                try:
                    if not short_name:
                        self.view.show_message("Enregistrement annulÃ© : nom vide.", "warning")
                        return

                    self.view.show_message("Enregistrement du short final...", "info")
                    final_output_path = shorts_dir / f"{short_name}.mp4"
                    
                    # Le fichier filtrÃ© existe dÃ©jÃ  (temp_filtered_path), on peut juste le renommer/copier !
                    # Mais attention, l'utilisateur veut peut-Ãªtre le short accÃ©lÃ©rÃ© ?
                    # Non, gÃ©nÃ©ralement un short est un extrait court, pas forcÃ©ment accÃ©lÃ©rÃ©.
                    # L'aperÃ§u Ã©tait accÃ©lÃ©rÃ© pour "voir vite".
                    # Si le but est d'avoir un short accÃ©lÃ©rÃ©, il faut garder l'accÃ©lÃ©ration.
                    # D'aprÃ¨s le code prÃ©cÃ©dent, le final Ã©tait en vitesse normale ('-c copy' depuis source).
                    # Donc on garde la vitesse normale.
                    
                    if temp_filtered_path.exists():
                        import shutil
                        shutil.move(str(temp_filtered_path), str(final_output_path))
                        self.view.show_message(f"Short '{short_name}.mp4' enregistrÃ© !", "success")
                    else:
                        raise Exception("Le fichier temporaire a disparu.")
                        
                except Exception as e:
                    self.view.show_message(f"Erreur enregistrement final: {e}", "error")

            else:
                self.view.show_message("Enregistrement annulÃ©.", "info")

        finally:
            # 7. Nettoyer les fichiers temporaires
            if temp_preview_path.exists():
                try:
                    temp_preview_path.unlink()
                except OSError: pass
            # Si le fichier filtrÃ© n'a pas Ã©tÃ© dÃ©placÃ© (ex: annulÃ©), on le supprime
            if temp_filtered_path.exists():
                try:
                    temp_filtered_path.unlink()
                except OSError: pass

    def on_crop(self):
        """Active l'outil de recadrage"""
        print("âœ‚ï¸ Outil de recadrage activÃ©")

    def _generer_miniature_video(self, chemin_video):
        """
        GÃ©nÃ¨re une miniature (QPixmap) Ã  partir de la premiÃ¨re frame d'une vidÃ©o.
        Args:
            chemin_video: Chemin vers le fichier vidÃ©o
        Returns:
            QPixmap ou None si Ã©chec
        """
        try:
            import cv2
            from PyQt6.QtGui import QImage, QPixmap
            cap = cv2.VideoCapture(chemin_video)
            if not cap.isOpened():
                print(f"âš ï¸ Impossible d'ouvrir la vidÃ©o : {chemin_video}")
                return None
            ret, frame = cap.read()
            cap.release()
            if not ret or frame is None:
                print(f"âš ï¸ Impossible de lire la premiÃ¨re frame : {chemin_video}")
                return None
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            height, width, channel = frame_rgb.shape
            bytes_per_line = 3 * width
            q_image = QImage(frame_rgb.data, width, height, bytes_per_line, QImage.Format.Format_RGB888)
            pixmap = QPixmap.fromImage(q_image)
            return pixmap
        except Exception as e:
            print(f"âŒ Erreur gÃ©nÃ©ration miniature pour {chemin_video}: {e}")
            return None